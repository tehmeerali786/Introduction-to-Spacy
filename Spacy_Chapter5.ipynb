{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z0f02VEpWRg"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBXQ9nWhpbw6"
      },
      "source": [
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgYu8OGxphkQ"
      },
      "source": [
        "doc = nlp('I want a green apple. ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCF_eVj2p7nw",
        "outputId": "577cae12-fe82-495a-ff3d-47c80b1b2bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "doc.similarity(doc[2:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7797949542342982"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj_KSdCsp_Uw",
        "outputId": "f478cffa-bf21-488b-a053-00bf952cae5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc.similarity(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPlM36ivqBvB",
        "outputId": "659a3f56-ad3a-4578-8f9d-8cd668e3d105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc[2:5].similarity(doc[2:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9xNFE_BqGtw"
      },
      "source": [
        "doc2 = nlp('I loke red oranges.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31cDdXkRr-oR",
        "outputId": "394615b5-746a-49e9-99fd-af4a09a534f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "doc2.similarity(doc[2:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24760853646925254"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTso915vsCuN",
        "outputId": "8b9ad452-b11b-495a-ce17-c6b4418ae3e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "token = doc[3:4]\n",
        "token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "green"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAJLoprxsXBc"
      },
      "source": [
        "token = doc2[3:4][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ4zbX8Psacs",
        "outputId": "ef1ed595-6070-4ac2-c3d5-6e00c722837f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "oranges"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrxBF2K7sbNc",
        "outputId": "a363b2b9-bc8f-44d3-c690-d0265eac1425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "token.similarity(doc[4:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.31554434"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLK-r3mUsj97",
        "outputId": "b3e3217e-01b1-4da8-fb94-562e74ac7fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc[4:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "apple"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUhmdBdisxJc",
        "outputId": "99ccc196-db05-4780-883a-1c9881a0e942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "token = nlp(u'fruits')[0]\n",
        "doc = nlp(u'I want to buy this beautiful book at the end of the week. Sales of citrus have increased over the last year. How much do you know this type of tree?')\n",
        "for sent in doc.sents:\n",
        "  print(sent.text)\n",
        "print('similarity to', token.text, 'is', token.similarity(sent), '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I want to buy this beautiful book at the end of the week.\n",
            "Sales of citrus have increased over the last year.\n",
            "How much do you know this type of tree?\n",
            "similarity to fruits is 0.13701063 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DdyVAdQbrxv",
        "outputId": "0ab37334-8cfe-40a5-b0cd-461b413a3ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "token = nlp(u'fruits')\n",
        "doc = nlp(u'I want to buy this beautiful book at the end of the week. Sales of citrus have increased over the last year. How much do you know this type of tree?')\n",
        "similarity = {}\n",
        "for i, sent in enumerate(doc.sents):\n",
        "  noun_span_list = [sent[j].text for j in range(len(sent)) if sent[j].pos_ == 'NOUN']\n",
        "  noun_span_str = ' '.join(noun_span_list)\n",
        "  noun_span_doc = nlp(noun_span_str)\n",
        "  similarity.update({i:token.similarity(noun_span_doc)})\n",
        "  print(similarity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0.18323929418823387}\n",
            "{0: 0.18323929418823387, 1: 0.40509359050466237}\n",
            "{0: 0.18323929418823387, 1: 0.40509359050466237, 2: 0.2766774169715806}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JqZLhoKhdjD",
        "outputId": "2759b07d-4565-45ee-f05d-ac583f89833b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i, j in similarity.items():\n",
        "  if i > 0:\n",
        "    temp = i-1\n",
        "    if j > similarity[temp]:\n",
        "      print(j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.40509359050466237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBPXNj0ynIjn",
        "outputId": "236607fb-7845-4ba3-eee0-f3990e3d732f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(similarity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLHvZZkwnMez",
        "outputId": "c42767a6-84bb-4157-c752-686c7969b637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# first sample text\n",
        "doc1 = nlp(u'Google Search, often reffered to as simply Google, is the most used search engine nowadays. It handles a huge number of searches each day.')\n",
        "\n",
        "# second sample text\n",
        "doc2 = nlp(u'Microsoft Windows is a family of proprietary operating systems developed and sold by Microsoft. The company also produces a wide range of other software for desktops and servers.')\n",
        "\n",
        "# third sample text\n",
        "doc3 = nlp(u'Tittica is a large, deep, mountain lake in the Andes. It is known as the highest navigable lake in the world.')\n",
        "\n",
        "docs = [doc1, doc2, doc3]\n",
        "\n",
        "spans = {}\n",
        "\n",
        "for j, doc in enumerate(docs):\n",
        "  named_entity_span = [doc[i].text for i in range(len(doc)) if doc[i].ent_type != 0]\n",
        "  print(named_entity_span)\n",
        "  named_entity_span = ' '.join(named_entity_span)\n",
        "  named_entity_span = nlp(named_entity_span)\n",
        "  spans.update({j:named_entity_span})\n",
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Google', 'Search', 'Google', 'each', 'day']\n",
            "['Microsoft', 'Windows', 'Microsoft']\n",
            "['Tittica', 'Andes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5o23W6nwDTQ",
        "outputId": "20342910-3262-46e1-974c-61d58582dd5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "spans"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: Google Search Google each day,\n",
              " 1: Microsoft Windows Microsoft,\n",
              " 2: Tittica Andes}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdJWFyWYwlOS",
        "outputId": "d011e277-cf68-44eb-8028-e53df8c94085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print('doc1 is similar to doc2:', spans[0].similarity(spans[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doc1 is similar to doc2: 0.5977375871782556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcxhfjWi1PE1",
        "outputId": "94967d5c-d726-444a-fe46-c6040bc117b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print('doc1 is similar to doc3:', spans[0].similarity(spans[2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doc1 is similar to doc3: 0.6682077574992572\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HljMysH1YtK",
        "outputId": "da83859f-18aa-4131-83b0-ba388b862bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print('doc2 is similar to doc3:', spans[1].similarity(spans[2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doc2 is similar to doc3: 0.6786278403773782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q5waxBQ1hpZ"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_JqkMxIPyxC"
      },
      "source": [
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ghbVHAyP40N"
      },
      "source": [
        "doc1 = nlp(u'We can overtake them.')\n",
        "doc2 = nlp(u'You must specify it.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DfuugNvQBxs",
        "outputId": "0eb633af-f2cd-4df1-f5ae-f969bada5bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for i in range(len(doc1)-1):\n",
        "  if doc1[i].dep_ == doc2[i].dep_:\n",
        "    print(doc1[i].text, doc2[i].text, doc1[i].dep_, spacy.explain(doc1[i].dep_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We You nsubj nominal subject\n",
            "can must aux auxiliary\n",
            "overtake specify ROOT None\n",
            "them it dobj direct object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ituAXBp-Qizb",
        "outputId": "f72f12f7-7bc1-4c0d-dd9b-b82e25b303fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for i in range(len(doc1)-1):\n",
        "  if doc1[i].pos_ == doc2[i].pos_:\n",
        "    print(doc1[i].text, doc2[i].text, doc1[i].pos_, spacy.explain(doc1[i].pos_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We You PRON pronoun\n",
            "can must VERB verb\n",
            "overtake specify VERB verb\n",
            "them it PRON pronoun\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bLEs_HSaWFf"
      },
      "source": [
        "doc = nlp(u'We can overtake them. You must specify it.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUumGoPigNbr"
      },
      "source": [
        "document = []\n",
        "for sent in doc.sents:\n",
        "  document.append(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE2d82Z0gcNQ",
        "outputId": "28cec2e8-6b04-4d27-92ec-5aeefc20b56f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(document)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXWMt7t6geX_",
        "outputId": "775e2131-0dc0-4602-b143-c43c94f6333e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "document[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "We can overtake them."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsabrERegiLP"
      },
      "source": [
        "length = len(document[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Q9ZvGci2W4",
        "outputId": "ebc2aa55-410a-4186-a1e2-c8678674b703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdZMpKyNgrZP",
        "outputId": "fc06c36e-5a67-4b88-ca78-0ee14219d5fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for i in range(len(document[0])-1):\n",
        "  if document[0][i].pos_ ==  document[1][i].pos_:\n",
        "    print(document[0][i].text, document[1][i].text, document[0][i].dep_, spacy.explain(document[0][i].dep_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We You nsubj nominal subject\n",
            "can must aux auxiliary\n",
            "overtake specify ROOT None\n",
            "them it dobj direct object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dKeDJwwUu7F"
      },
      "source": [
        "## **Chapter 6**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_VKFS4QU6BZ"
      },
      "source": [
        "### **Checking Utterance for a Pattern**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGWj3WnWU5_S"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMa5C1rniwzS"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGf0nDuSUr8L"
      },
      "source": [
        "def dep_pattern(doc):\n",
        "  for i in range(len(doc)-1):\n",
        "    if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and doc[i+2].dep_ == 'ROOT':\n",
        "      for tok in doc[i+2].children:\n",
        "        if tok.dep_ == 'dobj':\n",
        "          return True\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz_JQDVlZ5Qq",
        "outputId": "c35fdae3-399a-491c-8acc-e81f653f73b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc = nlp(u'We can overtake them.')\n",
        "if dep_pattern(doc):\n",
        "  print('Found')\n",
        "else:\n",
        "  print('Not Found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wah2pGF_cyY6"
      },
      "source": [
        "## **Using Spacy Matcher to Find Word Sequence Patterns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXOQqgpvaTKl",
        "outputId": "4122d8ae-7131-48be-c6b9-148bc3303749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\"}, {\"DEP\": \"ROOT\"}]\n",
        "matcher.add(\"NsubjAuxRoot\", None, pattern)\n",
        "doc = nlp(u\"We can overtake them.\")\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "  span = doc[start:end]\n",
        "  print(\"Span: \", span.text)\n",
        "  print(\"The positions in the doc are: \", start, \"-\", end)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Span:  We can overtake\n",
            "The positions in the doc are:  0 - 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33SkD0-mffPR",
        "outputId": "25780995-4973-4282-e038-5554d6dcad6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(10599197345289971701, 0, 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBR2iooWOVqd"
      },
      "source": [
        "## **Applying Several Patterns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU_32a79gX-6"
      },
      "source": [
        "# Insert the dep_pattern function from a previous listing here\n",
        "# ........\n",
        "\n",
        "def pos_pattern(doc):\n",
        "  for token in doc:\n",
        "    if token.dep_ == 'nsubj' and token.tag_ != 'PRP':\n",
        "      return False\n",
        "    if token.dep_ == 'aux' and token.tag_ != 'MD':\n",
        "      return False\n",
        "    if token.dep_ == 'ROOT' and token.tag_ != 'VB':\n",
        "      return False\n",
        "    if token.dep_ == 'dobj' and token.tag_ != 'PRP':\n",
        "      return False\n",
        "\n",
        "  return True\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W32Foau7PlVT",
        "outputId": "e584d896-5285-491b-d5ab-4420f2365dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Code\n",
        "doc = nlp(u'We can overtake them.')\n",
        "if dep_pattern(doc) and pos_pattern(doc):\n",
        "  print('Found')\n",
        "else:\n",
        "  print('Not Found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BE5i7uKQApo",
        "outputId": "8d5c2beb-f9bf-4f7a-db9f-f21021847830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Code\n",
        "doc = nlp(u'I might send them a card as a reminder.')\n",
        "if dep_pattern(doc) and pos_pattern(doc):\n",
        "  print('Found')\n",
        "else:\n",
        "  print('Not Found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not Found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_NcgO7Rg4GT"
      },
      "source": [
        "## **Creating Patterns Based on Customized Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibrozbe7Q13H"
      },
      "source": [
        "# Insert the dep_pattern and pos_pattern functions from previous listings here......\n",
        "\n",
        "def pron_pattern(doc):\n",
        "  plural = ['we', 'us', 'they', 'them']\n",
        "  for token in doc:\n",
        "    if token.dep_ == 'dobj' and token.tag_ == 'PRP':\n",
        "      if token.text in plural:\n",
        "        return 'plural'\n",
        "      else:\n",
        "        return 'singular'\n",
        "\n",
        "  return 'not found'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sm1xp6CiZZW",
        "outputId": "9f2aa5b9-bac7-4d00-db57-a1085547c3a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc = nlp(u'We can overtake them.')\n",
        "if dep_pattern(doc) and pos_pattern(doc):\n",
        "  print('Found: ', 'the pronoun is in position of direct object is ', pron_pattern(doc))\n",
        "else:\n",
        "  print('Not Found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found:  the pronoun is in position of direct object is  plural\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dtLp7zejgu8",
        "outputId": "68f81d81-875d-4eb0-9862-9c3375600666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc = nlp(u'Developers might follow this rule.')\n",
        "dep_pattern(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi9XKxubq7ih"
      },
      "source": [
        "## **Using Word Sequence Patterns in Chatbotts to Generate Statements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyP6qsW8ncuI"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# Insert the dep_pattern, pos_pattern and pron_pattern functions from the previous listings here.\n",
        "\n",
        "def find_noun(sents, num):\n",
        "  if num == 'plural':\n",
        "    taglist = ['NNS', 'NNPS']\n",
        "    if num == 'singular':\n",
        "      taglist = ['NN', 'NNP']\n",
        "    for sent in reversed(sents):\n",
        "      for token in sent:\n",
        "        if token.tag_ in taglist:\n",
        "          return token.text\n",
        "    return 'Noun not found'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hpSvWY0sPaR"
      },
      "source": [
        "def gen_utterance(doc, noun):\n",
        "  sent = ''\n",
        "  for i, token in enumerate(doc):\n",
        "    if token.dep_ == 'dobj' and token.tag_ == 'PRP':\n",
        "      sent = doc[:i].text + ' ' + noun + ' ' + doc[i+1:len(doc)-2].text + 'too.'\n",
        "      return sent\n",
        "\n",
        "  return 'Failed to generate utterance'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lHTl_qIuuGz",
        "outputId": "d151db25-0586-476a-9c44-030a9be3b578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "doc = nlp(u'The symbols are clearly distinguishable. I can recognize them prompltly.')\n",
        "sents = list(doc.sents)\n",
        "response = ''\n",
        "noun = ''\n",
        "\n",
        "for i, sent in enumerate(sents):\n",
        "  if dep_pattern(sent) and pos_pattern(sent):\n",
        "    noun = find_noun(sents[:i], pron_pattern(sent))\n",
        "    if noun != 'Noun not found!':\n",
        "      response = gen_utterance1(sents[i], noun)\n",
        "      break\n",
        "\n",
        "print(response)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 I\n",
            "nsubj\n",
            "1 can\n",
            "aux\n",
            "2 recognize\n",
            "ROOT\n",
            "3 them\n",
            "dobj\n",
            "4 prompltly\n",
            "advmod\n",
            "5 .\n",
            "punct\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gf3Y07Jwap0"
      },
      "source": [
        "def gen_utterance1(doc, noun):\n",
        "  sent = ''\n",
        "  for i, token in enumerate(doc):\n",
        "    print(i, token)\n",
        "    print(token.dep_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb6jLNyOxpji",
        "outputId": "29728ce5-eb76-463e-d1bd-b29587d4f14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "doc = nlp(u'The symbols are clearly distinguishable. I can recognize them prompltly.')\n",
        "sents = list(doc.sents)\n",
        "response = ''\n",
        "noun = ''\n",
        "\n",
        "for i, sent in enumerate(sents):\n",
        "  if dep_pattern(sent) and pos_pattern(sent):\n",
        "    noun = find_noun(sents[:i], pron_pattern(sent))\n",
        "    if noun != 'Noun not found!':\n",
        "      response = gen_utterance2(sents[i], noun)\n",
        "      break\n",
        "\n",
        "print(response)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 them\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6Shc6qBzEts"
      },
      "source": [
        "def gen_utterance2(doc, noun):\n",
        "  sent = ''\n",
        "  for i, token in enumerate(doc):\n",
        "    if token.dep_ == 'dobj' and token.tag_ == 'PRP':\n",
        "      print(i, token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_DRFqjCz1me",
        "outputId": "c495bc47-affe-416f-ecf2-bec17ae00c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc = nlp(u'The symbols are clearly distinguishable. I can recognize them prompltly.')\n",
        "sents = list(doc.sents)\n",
        "response = ''\n",
        "noun = ''\n",
        "\n",
        "for i, sent in enumerate(sents):\n",
        "  if dep_pattern(sent) and pos_pattern(sent):\n",
        "    noun = find_noun(sents[:i], pron_pattern(sent))\n",
        "    if noun != 'Noun not found!':\n",
        "      response = gen_utterance(sents[i], noun)\n",
        "      break\n",
        "\n",
        "print(response)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I can recognize symbols too.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAU6oQ84Bo_I"
      },
      "source": [
        "# **Extracting Keywords from Syntactic Dependency Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG5Bn9LtBo6Y"
      },
      "source": [
        "## **Walking a Dependency Tree for Information Extraction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzhenq5kCAsI"
      },
      "source": [
        "## **Iterating Over the Heads of Tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTnGUORQ0Dx-"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# Here's the function that figures out the destination\n",
        "def det_destination(doc):\n",
        "  for i, token in enumerate(doc):\n",
        "    if token.ent_type !=0 and token.ent_type_ == 'GPE':\n",
        "      while True:\n",
        "        token = token.head\n",
        "        if token.text == 'to':\n",
        "          return doc[i].text\n",
        "        if token.head == token:\n",
        "          return 'Failed to determine'\n",
        "  return 'Failed to determine'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxJmSa3VHf-G",
        "outputId": "742286de-9467-4615-d422-512303d8ab29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing the det_destination function\n",
        "doc = nlp(u'I am going to the conference in Berlin.')\n",
        "dest = det_destination(doc)\n",
        "print('It seems the user wants a ticket to ' + dest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It seems the user wants a ticket to Berlin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_VVTc2FIN4O"
      },
      "source": [
        "def det_destination1(doc):\n",
        "  for i, token in enumerate(doc):\n",
        "    print(i, token, token.ent_type_)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QuZPhClJXKY",
        "outputId": "4db50d81-56df-4481-b45e-694f8b88e1fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "doc = nlp(u'I am going to the conference in Berlin.')\n",
        "det_destination1(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 I \n",
            "1 am \n",
            "2 going \n",
            "3 to \n",
            "4 the \n",
            "5 conference \n",
            "6 in \n",
            "7 Berlin GPE\n",
            "8 . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk32Pr-vZeuh"
      },
      "source": [
        "## **Condensing a Text Using Dependency Trees**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW2PfYx2Jdno"
      },
      "source": [
        "doc = nlp(u'The product sales hit a new record in the first quarter, with 18.6 million units sold.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pur2hJMKZzFf",
        "outputId": "edae716d-e3fc-4ed5-bc79-0a1623741d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "phrase = ''\n",
        "\n",
        "for token in doc:\n",
        "  if token.pos_ == 'NUM':\n",
        "    while True:\n",
        "      phrase = phrase + ' ' + token.text\n",
        "      token = token.head\n",
        "      if token not in list(token.head.lefts):\n",
        "        phrase = phrase + ' ' + token.text\n",
        "        break\n",
        "\n",
        "    break\n",
        "print(phrase.strip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18.6 million units\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a42H8guAcToG",
        "outputId": "620a32f2-c10e-4f68-a326-48532c1f0013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "while True:\n",
        "  print(token)\n",
        "  token = doc[token.i].head\n",
        "  print(token)\n",
        "  if token.pos_ != 'ADP':\n",
        "    phrase = token.text + phrase\n",
        "    print(phrase)\n",
        "  if token.dep_ == 'ROOT':\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "units\n",
            "with\n",
            "with\n",
            "hit\n",
            "hit 18.6 million units\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtPfYbjEebdy",
        "outputId": "9f862420-7c05-4557-9727-1a4e94fb4f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "phrase"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hit 18.6 million units'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqKBrVihfpS3",
        "outputId": "edc07704-0cde-4958-cfa5-f0fc5278042a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for tok in token.lefts:\n",
        "  if tok.dep_ == 'nsubj':\n",
        "    phrase = ' '.join([tok.text for tok in tok.lefts]) + ' ' + tok.text + ' ' + phrase\n",
        "    break\n",
        "\n",
        "print(phrase)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The product sales hit 18.6 million units\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ4f-FVThUtb"
      },
      "source": [
        "doc = nlp(u'The company, whose profits reached a record high this year, largely attributed to changes in management, earned a total revenue of $4.26 million.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So-q_kD8lVLO",
        "outputId": "28b9cd2d-ec23-4c2e-a85c-79e53b5c29ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "phrase = ''\n",
        "\n",
        "for token in doc:\n",
        "  if token.pos_ == 'NUM':\n",
        "    while True:\n",
        "      phrase = phrase + ' ' + token.text\n",
        "      token = token.head\n",
        "      if token not in list(token.head.lefts):\n",
        "        phrase = phrase + ' ' + token.text\n",
        "        break\n",
        "\n",
        "    break\n",
        "print(phrase.strip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.26 million\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9rPO7X-lcEi",
        "outputId": "05e73735-9d4c-4e87-a8a6-473aacb45493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "while True:\n",
        "  print(token)\n",
        "  token = doc[token.i].head\n",
        "  print(token)\n",
        "  phrase = token.text + ' '+ phrase\n",
        "  print(phrase)\n",
        "  if token.dep_ == 'ROOT':\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "million\n",
            "of\n",
            "of  4.26 million\n",
            "of\n",
            "revenue\n",
            "revenue of  4.26 million\n",
            "revenue\n",
            "earned\n",
            "earned revenue of  4.26 million\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST2sPw-wl2Rh",
        "outputId": "ccfa36f6-6a96-4a8b-97ab-838134d16053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for tok in token.lefts:\n",
        "  if tok.dep_ == 'nsubj':\n",
        "    phrase = ' '.join([tok.text for tok in tok.lefts]) + ' ' + tok.text + ' ' + phrase\n",
        "    break\n",
        "\n",
        "print(phrase)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The company earned revenue of  4.26 million\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfhrTDUa673z"
      },
      "source": [
        "# **Using Context to Improve the Ticket-Booking Chatbot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fj1AYV7mndf"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p29nA7-u7aet"
      },
      "source": [
        "# Insert the dep_destination function from a previous listing here\n",
        "# .......\n",
        "\n",
        "def guess_destination(doc):\n",
        "  for token in doc:\n",
        "    if token.ent_type != 0 and token.ent_type == 'GPE':\n",
        "      return token.text\n",
        "\n",
        "  return 'Failed to determine'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAni6GlB8zbO"
      },
      "source": [
        "def gen_response(doc):\n",
        "  dest = det_destination(doc)\n",
        "  if dest != 'Failed to determine':\n",
        "    return 'When do you need to be in ' + dest + '?'\n",
        "  dest = guess_destination(doc)\n",
        "  if dest != 'Failed to determine':\n",
        "    return 'You want a ticket to ' + dest + ', right?'\n",
        "\n",
        "  return 'Are you flying somewhere?'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIoCIunX-88h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "241d7986-82d2-4f02-abbe-5bb9b160c4b7"
      },
      "source": [
        "doc = nlp(u'I am going to the conference in Berlin.')\n",
        "print(gen_response(doc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When do you need to be in Berlin?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmzJunlKElog"
      },
      "source": [
        "# **Making a Smarter Chatbot by Finding Proper Modifiers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlhhGSfJAIO3"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q5ZsvNPE8bA"
      },
      "source": [
        "doc = nlp(u'Kiwano has jelly-like flesh with a refreshingly fruity taste. This is a nice exotic fruit from Africa. It is definitely worth trying.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdi6bGGDFI7q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "80012fbc-9662-4981-ef78-446161f6f86d"
      },
      "source": [
        "fruit_adjectives = []\n",
        "fruit_origins = []\n",
        "\n",
        "for token in doc:\n",
        "  if token.text == 'fruit':\n",
        "    fruit_adjectives = fruit_adjectives + [modifier.text for modifier in token.lefts if modifier.pos_ == 'ADJ']\n",
        "    fruit_origins = fruit_origins + [doc[modifier.i + 1].text for modifier in token.rights if modifier.text == 'from' and doc[modifier.i + 1].ent_type !=0]\n",
        "print('This list adjectival modifiers for word fruit: ', fruit_adjectives)\n",
        "print('The list of GPE names applicable to word fruit as postmodifiers:', fruit_origins)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This list adjectival modifiers for word fruit:  ['nice', 'exotic']\n",
            "The list of GPE names applicable to word fruit as postmodifiers: ['Africa']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNZf_JQqHpf7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}